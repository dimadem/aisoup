{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothetical Document Embedding - [HyDE]:\n",
    "\n",
    "HyDE is an innovative approach that transforms query questions into hypothetical documents containing the answer, aiming to bridge the gap between query and document distributions in vector space.\n",
    "\n",
    "\n",
    "In this tutorial, we will focus on the following topics:\n",
    "- Generate a hypothetical document from the query using the language model.\n",
    "- Use the hypothetical document as the search query in the vector store.\n",
    "- Retrieve the most similar documents to this hypothetical document.\n",
    "\n",
    "---\n",
    "tools:\n",
    "* [anthropic](https://github.com/anthropics/anthropic-sdk-python)\n",
    "* [chromadb](https://github.com/chroma-core/chroma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# load .env\n",
    "load_dotenv(dotenv_path='../.env')\n",
    "\n",
    "# get API key\n",
    "api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "\n",
    "print(\"api_key -> \", api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import pprint\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# declare default embedding function [all-MiniLM-L6-v2]\n",
    "default_embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "collection_name = \"hyde_collection\"\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chromadb/\")\n",
    "# declare ChromaDB collection\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=collection_name,\n",
    "    embedding_function=default_embedding_function\n",
    "    )\n",
    "\n",
    "result = collection.get()\n",
    "\n",
    "print(f\"Collection {collection_name} created successfully\")\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "def load_txt_from_dir(dir_path):\n",
    "    documents = []\n",
    "    for filename in os.listdir(dir_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(dir_path, filename), \"r\") as file:\n",
    "                documents.append({\"text\": file.read()})\n",
    "    return documents\n",
    "\n",
    "directory_path = \"../files/txt\"\n",
    "\n",
    "# load documents from directory\n",
    "txt_files = load_txt_from_dir(directory_path)\n",
    "\n",
    "print(f\" {len(txt_files)} files loaded\")\n",
    "pprint.pprint(txt_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "def split_text(\n",
    "    text, \n",
    "    chunk_size=200, \n",
    "    chunk_overlap=20\n",
    "    ):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_length = len(text)\n",
    "    while start < text_length:\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start = end - chunk_overlap\n",
    "    return chunks\n",
    "\n",
    "# split text into chunks\n",
    "chunked_txt = []\n",
    "\n",
    "for file_id, txt_file in enumerate(txt_files):\n",
    "    chunks = split_text(txt_file[\"text\"])\n",
    "    for chunk_id, chunk in enumerate(chunks):\n",
    "        chunked_txt.append(\n",
    "            {\n",
    "                'id': f\"{file_id}-{chunk_id}\", \n",
    "                'text': chunk,\n",
    "            }\n",
    "        )\n",
    "\n",
    "print(f\"Split in to {len(chunked_txt)} chunks\\n\\n\")\n",
    "\n",
    "pprint.pprint(chunked_txt[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsert documents with embeddings to collection ChromaDB\n",
    "for chunk in chunked_txt :\n",
    "    collection.upsert(\n",
    "            ids=chunk['id'],\n",
    "            documents=chunk['text'],\n",
    "    )\n",
    "\n",
    "result = collection.get()\n",
    "\n",
    "print(f\"Collection {collection_name} has {len(result['ids'])} documents\")\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Augmented query generation\n",
    "\n",
    "<img width=50% src=\"https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/d5cf5b72cb213da587c7fac4d14766ad890177fb/images/HyDe.svg\">\n",
    "\n",
    "\n",
    "Use cases:\n",
    "- Information Retrieval\n",
    "- Question Answering Systems\n",
    "- E-commerce Search\n",
    "- Academic Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HYPOTHETICAL ANSWER <- Augment query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "client = Anthropic(api_key=api_key)\n",
    "\n",
    "def augment_query(query):\n",
    "    prompt = \"Provide an hypothetical answer to the given question.\"\n",
    "    user_prompt = query + \"\\n\" + prompt\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=1024,\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_prompt\n",
    "        }]\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "query = \"How much cost synthesizer Labyrinth?\"\n",
    "augment = augment_query(query)\n",
    "print(augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "# function to query collection\n",
    "def query_collection(question, n_results=5):\n",
    "    results = collection.query(\n",
    "        query_texts=question,\n",
    "        n_results=n_results,\n",
    "        # include=['embeddings', 'documents', 'distances']\n",
    "    )\n",
    "    # pprint.pprint(results)\n",
    "    \n",
    "    # extract relevant chunks\n",
    "    relevant_chunks = [txt for sublist in results[\"documents\"] for txt in sublist]\n",
    "    # pprint.pprint(relevant_chunks)\n",
    "    # for idx, txt in enumerate(results[\"documents\"]):\n",
    "    #     txt_id = results[\"ids\"][0][idx]\n",
    "    #     distance = results[\"distances\"][0][idx]\n",
    "    #     print(\"Chunks found:\")\n",
    "    #     print(f\"document id: {txt_id}\")\n",
    "    #     print(f\"text found:  {txt}\")\n",
    "    #     print(f\"distance:    {distance}\\n\\n\")\n",
    "\n",
    "    return relevant_chunks\n",
    "\n",
    "\n",
    "# function for generate response with openai\n",
    "def api_response(query, relevant_chunks):\n",
    "    \n",
    "    context = \"\\n\\n\".join(relevant_chunks)\n",
    "    \n",
    "    user_prompt = (f\"\"\"\n",
    "            You have been tasked with helping us to answer the following query: \n",
    "            <query>\n",
    "            {query}\n",
    "            </query>\n",
    "            You have access to the following documents which are meant to provide context as you answer the query:\n",
    "            <documents>\n",
    "            {context}\n",
    "            </documents>\n",
    "            Please remain faithful to the underlying context, and only deviate from it if you are 100% sure that you know the answer already. \n",
    "            Answer the question now, and avoid providing preamble such as 'Here is the answer', etc\n",
    "            \"\"\"\n",
    "            )\n",
    "    \n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=2048,\n",
    "        messages=[{\n",
    "                \"role\": \"user\", \n",
    "                \"content\": user_prompt\n",
    "                }],\n",
    "        temperature=0\n",
    "\n",
    "    )\n",
    "    return response.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query collection\n",
    "\n",
    "question = \"tell me how much cost Labyrinth?\"\n",
    "augmented_result = augment_query(question)\n",
    "relevant_chunks = query_collection(question + \"\\n\" + augmented_result)\n",
    "answer = api_response(question, relevant_chunks)\n",
    "print(\"\\n------------------------------------\\n\")\n",
    "print(\"answer ->\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "well done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Good things to know\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's cleanup db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### list collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_collections = chroma_client.list_collections()\n",
    "\n",
    "print(list_collections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### delete collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client.delete_collection(collection_name)\n",
    "\n",
    "list_collections = chroma_client.list_collections()\n",
    "\n",
    "print(list_collections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "made with <3 by \n",
    "[dima dem](https://github.com/dimadem/) |42London"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
