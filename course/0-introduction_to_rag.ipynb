{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to RAG:\n",
    "\n",
    "What is **RAG**?\n",
    "\n",
    "RAG is a concept in natural language processing (NLP) and artificial intelligence (AI) that represents a framework for building conversational AI models. RAG stands for Retrieve, Augment, and Generate, which are the three primary components of this framework.\n",
    "\n",
    "- **Retrieve:** This component involves retrieving relevant information from a knowledge base, database, or external sources. The goal is to gather context and background information related to the user's query or input.\n",
    "- **Augment:** In this component, the retrieved information is augmented with additional data, such as user preferences, conversation history, or external knowledge. This step helps to refine the understanding of the user's intent and provides more context for generating a response.\n",
    "- **Generate:** The final component involves generating a response based on the retrieved and augmented information. This response is typically generated using a machine learning model, such as a language model or a text generator.\n",
    "\n",
    "#### **Why RAG is needed:**\n",
    "**Improved accuracy:** RAG helps to improve the accuracy of conversational AI models by providing more context and relevant information.\n",
    "\n",
    "**Personalization:** By incorporating user preferences and conversation history, RAG enables more personalized responses.\n",
    "\n",
    "**Efficient knowledge retrieval:**\n",
    "RAG allows for efficient retrieval of knowledge from external sources, reducing the need for manual knowledge updates.\n",
    "\n",
    "**Flexibility:** \n",
    "The RAG framework can be applied to various conversational AI applications, such as chatbots, voice assistants, and language translation systems.\n",
    "\n",
    "---\n",
    "**What we do there:**\n",
    "- API call to LLM\n",
    "- Building the tiniest RAG\n",
    "\n",
    "tools:\n",
    "- anthropic\n",
    "\n",
    "\n",
    "### Take a look at\n",
    "\n",
    "[Anthropic cookbook](https://github.com/anthropics/anthropic-cookbook/)\n",
    "\n",
    "[Anthropic api reference](https://docs.anthropic.com/en/api/getting-started)\n",
    "\n",
    "### Python API\n",
    "\n",
    "[Getting started](https://github.com/anthropics/courses/blob/master/anthropic_api_fundamentals/01_getting_started.ipynb)\n",
    "\n",
    "[Messages format](https://github.com/anthropics/courses/blob/master/anthropic_api_fundamentals/02_messages_format.ipynb)\n",
    "\n",
    "[Models](https://github.com/anthropics/courses/blob/master/anthropic_api_fundamentals/03_models.ipynb)\n",
    "\n",
    "[Parameters](https://github.com/anthropics/courses/blob/master/anthropic_api_fundamentals/04_parameters.ipynb)\n",
    "\n",
    "[Streaming](https://github.com/anthropics/courses/blob/master/anthropic_api_fundamentals/05_Streaming.ipynb)\n",
    "\n",
    "[Vision](https://github.com/anthropics/courses/blob/master/anthropic_api_fundamentals/06_vision.ipynb)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üçú New ingredients!\n",
    "\n",
    "- os - module, which provides functions for interacting with the operating system.\n",
    "- load_dotenv - function from the dotenv library, which loads environment variables from a .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# load .env\n",
    "load_dotenv(dotenv_path='../.env')\n",
    "\n",
    "# get API key\n",
    "api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "\n",
    "print(\"api_key -> \", api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### llm know everything?\n",
    "\n",
    "let's check can LLM model know what is current day?\n",
    "\n",
    "* read content from system role\n",
    "* read content from user role\n",
    "\n",
    "---\n",
    "\n",
    "### üçú New ingredients!\n",
    "\n",
    "##### Role\n",
    "When using Claude, you can dramatically improve its performance by using the system parameter to give it a role. This technique, known as role prompting, is the most powerful way to use system prompts with Claude.\n",
    "\n",
    "The right role can turn Claude from a general assistant into your virtual domain expert!\n",
    "\n",
    "\n",
    "##### Content\n",
    "The content is the actual information or message being passed from one party to another. \n",
    "\n",
    "##### Good to read\n",
    "\n",
    "[Giving Claude a role with a system prompt](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/system-prompts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "client = Anthropic(api_key=api_key)\n",
    "\n",
    "system_prompt = \"You are the helpful assistant. If you don't know the answer, please respond with 'I don't know'.\"\n",
    "\n",
    "user_prompt = \"What is the current date?\"\n",
    "\n",
    "# make a response\n",
    "response = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    system=system_prompt,\n",
    "    max_tokens=1024,\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt\n",
    "    }]\n",
    ")\n",
    "\n",
    "# print the response\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### current date\n",
    "\n",
    "add some knowledge to our context.\n",
    "\n",
    "here we use function which returns current date.\n",
    "\n",
    "---\n",
    "\n",
    "### üçú New ingredients!\n",
    "\n",
    "<b>Retrieve</b> - we put function <b>date.today()</b> as a context with relevant data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "from datetime import date\n",
    "\n",
    "client = Anthropic(api_key=api_key)\n",
    "\n",
    "system_prompt = \"You are the helpful assistant. If you don't know the answer, please respond with 'I don't know'.\"\n",
    "\n",
    "question = \"What is the current date?\"\n",
    "\n",
    "user_prompt = (f\"You know that the current date is {date.today()}\"\n",
    "            + question)\n",
    "\n",
    "# make a response\n",
    "response = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    system=system_prompt,\n",
    "    max_tokens=1024,\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(\"LLM      -> \", response.content[0].text)\n",
    "print(\"function -> \", date.today())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### specific text\n",
    "\n",
    "let's add some custom text and ask about data in it\n",
    "\n",
    "---\n",
    "\n",
    "### üçú New ingredients!\n",
    "\n",
    "<b>query</b> - question we give\n",
    "\n",
    "<b>context</b> - relevant date we have\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "from datetime import date\n",
    "\n",
    "client = Anthropic(api_key=api_key)\n",
    "\n",
    "context = \"Labyrinth is available now for ¬£599.\"\n",
    "\n",
    "query = \"How much cost Labyrinth?\"\n",
    "\n",
    "user_prompt = (f\"\"\"\n",
    "            You have been tasked with helping us to answer the following query: \n",
    "            <query>\n",
    "            {query}\n",
    "            </query>\n",
    "            You have access to the following documents which are meant to provide context as you answer the query:\n",
    "            <documents>\n",
    "            {context}\n",
    "            </documents>\n",
    "            Please remain faithful to the underlying context, and only deviate from it if you are 100% sure that you know the answer already. \n",
    "            Answer the question now, and avoid providing preamble such as 'Here is the answer', etc\n",
    "            \"\"\"\n",
    "            )\n",
    "\n",
    "# make a response\n",
    "response = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=1024,\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(\"LLM      -> \", response.content[0].text)\n",
    "print(\"context  -> \", context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### well done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Conclusion\n",
    "In this tutorial, we look at all three key components of the RAG pipeline:\n",
    "1. We create a small database that contains just a string with specific knowledge about cost, and we use the whole string as a relevant chunk, utilizing the \"**Retrieve**\" component to extract relevant information.\n",
    "2. We **Augment** the information with a specific prompt that returns 'null' if there is no relevant data.\n",
    "3. We use an API call to LLM with the whole prompt to **Generate** a response.\n",
    "\n",
    "\n",
    "In the next tutorial, we will use a Vector Database to retrieve specific parts with relevant information that fit the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "made with <3 by \n",
    "[dima dem](https://github.com/dimadem/) |42London"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
